âœ… Goal
Enable each Message Node to optionally connect to your existing AI Assistant to generate responses dynamically using OpenAI â€” including text, quick replies, or carousels â€” based on user context or a system-defined prompt.

ğŸ§  Key Additions to the System
1. ğŸ”Œ AI Mode Toggle in Node Editor
Add a new field to the Edit Node popup:

plaintext
Copy
Edit
[ ] Enable AI Mode (Use Assistant)
When toggled ON:

Message Type becomes auto-generated via AI Assistant.

Manual content fields (text, quick replies, etc.) are disabled or optional fallbacks.

2. ğŸ”— Assistant Selection
Once AI Mode is ON, show:

plaintext
Copy
Edit
Assistant to use: [Dropdown populated from existing assistants]
âœ… Pull options from Assistants section in AI Chatbot Management.

3. ğŸ§  Prompt Logic for the Assistant
System sends a structured message to the selected assistant:

Example Prompt to Assistant:

plaintext
Copy
Edit
User has entered the chat at the "Welcome" step. 
Generate a short greeting message and 2â€“4 clickable options (quick replies) relevant to a food delivery service chatbot. 
Language: Swedish
User context: {location: Stockholm, previous input: "Do you deliver after 9PM?"}
The assistant responds with:

json
Copy
Edit
{
  "message": "Ja, vi levererar till klockan 22. Vad vill du gÃ¶ra hÃ¤rnÃ¤st?",
  "quickReplies": ["Se meny", "Ã–ppettider", "BestÃ¤ll nu"]
}
4. ğŸ“¦ Node Response Injection
After assistant returns data:

Use the response to populate the chatbot dynamically when the node is triggered in real-time.

Optional: Cache generated replies for preview/testing.

ğŸ§° Technical Integration Steps
Frontend:

Add toggle + assistant selector to the node editor modal.

Show â€œLive Previewâ€ of AI-generated content if available.

Backend:

On node trigger, if AI Mode is active:

Fetch assistant by ID from Assistants config

Construct prompt using node metadata + user session context

Send to OpenAI via your existing assistant routing

Parse and inject response into chat stream

ğŸ›¡ï¸ Safeguards
Fallback to default message if AI fails

Limit generation frequency per session to reduce API calls

Maintain multilingual support (prompt should contain lang flag)

